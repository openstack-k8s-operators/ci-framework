---
# source: dcn/service-values.yaml.j2
apiVersion: v1
kind: ConfigMap
metadata:
  name: service-values
  annotations:
    config.kubernetes.io/local-config: "true"
data:
  preserveJobs: false
  cinder:
    customServiceConfig: |
      [DEFAULT]
      storage_availability_zone = az0
  cinderAPI:
    replicas: 3
  cinderBackup:
    replicas: 3
    customServiceConfig: |
      [DEFAULT]
      backup_driver = cinder.backup.drivers.ceph.CephBackupDriver
      backup_ceph_conf = /etc/ceph/az0.conf
      backup_ceph_pool = backups
      backup_ceph_user = openstack
  cinderVolumes:
{% for _ceph in _ceph_vars_list %}
{% if _ceph.cifmw_ceph_client_cluster != _az_to_scaledown %}
    {{ _ceph.cifmw_ceph_client_cluster }}:
      customServiceConfig: |
        [DEFAULT]
        enabled_backends = ceph
        glance_api_servers = https://glance-{{ _ceph.cifmw_ceph_client_cluster }}-internal.openstack.svc:9292
        [ceph]
        volume_backend_name = ceph
        volume_driver = cinder.volume.drivers.rbd.RBDDriver
        rbd_ceph_conf = /etc/ceph/{{ _ceph.cifmw_ceph_client_cluster }}.conf
        rbd_user = openstack
        rbd_pool = volumes
        rbd_flatten_volume_from_snapshot = False
        rbd_secret_uuid = {{ _ceph.cifmw_ceph_client_fsid }}
        rbd_cluster_name = {{ _ceph.cifmw_ceph_client_cluster }}
        backend_availability_zone = {{ _ceph.cifmw_ceph_client_cluster }}
{% endif %}
{% endfor %}
  galera:
    templates:
      openstack:
        replicas: 1
        secret: osp-secret
        storageRequest: 5G
{% for index in range(1, _all_azs | length + 1) %}
{% if "az" ~ (index - 1) != _az_to_scaledown %}
      openstack-cell{{ index }}:
        replicas: 1
        secret: osp-secret
        storageRequest: 5G
{% endif %}
{% endfor %}
  glance:
    keystoneEndpoint: az0
    glanceAPIs:
{% for _ceph in _ceph_vars_list %}
{% if _ceph.cifmw_ceph_client_cluster != _az_to_scaledown %}
      {{ _ceph.cifmw_ceph_client_cluster }}:
        customServiceConfig: |
          [DEFAULT]
          enabled_import_methods = [web-download,copy-image,glance-direct]
          enabled_backends = {{ ci_dcn_site_glance_map[_ceph.cifmw_ceph_client_cluster]
                                | reject('equalto', _az_to_scaledown)
                                | join(':rbd,') + ':rbd' }}
          [glance_store]
          default_backend = {{ _ceph.cifmw_ceph_client_cluster }}
{% for _ceph_az in ci_dcn_site_glance_map[_ceph.cifmw_ceph_client_cluster] %}
{% if _ceph_az != _az_to_scaledown %}
          [{{ _ceph_az }}]
          rbd_store_ceph_conf = /etc/ceph/{{ _ceph_az }}.conf
          store_description = "{{ _ceph_az }} RBD backend"
          rbd_store_pool = images
          rbd_store_user = openstack
          rbd_thin_provisioning = True
{% endif %}
{% endfor %}
        networkAttachments:
          - storage
        override:
          service:
            internal:
              metadata:
                annotations:
                  metallb.universe.tf/address-pool: internalapi
                  metallb.universe.tf/allow-shared-ip: internalapi
                  metallb.universe.tf/loadBalancerIPs: 172.17.0.8{{ loop.index0 }}
              spec:
                type: LoadBalancer
{% if _ceph.cifmw_ceph_client_cluster == 'az0' %}
        replicas: 3
        type: split
{% else %}
        replicas: 1
        type: edge
{% endif %}
{% endif %}
{% endfor %}
  manila:
    customServiceConfig: |
      [DEFAULT]
      storage_availability_zone = az0
    enabled: true
    manilaAPI:
      customServiceConfig: |
        [DEFAULT]
        enabled_share_protocols=nfs,cephfs
    manilaShares:
{% for _ceph in _ceph_vars_list %}
{% if _ceph.cifmw_ceph_client_cluster != _az_to_scaledown %}
      {{ _ceph.cifmw_ceph_client_cluster }}:
        customServiceConfig: |
          [DEFAULT]
          enabled_share_backends = cephfs_{{ _ceph.cifmw_ceph_client_cluster }}
          enabled_share_protocols = cephfs
          [cephfs_{{ _ceph.cifmw_ceph_client_cluster }}]
          driver_handles_share_servers = False
          share_backend_name = cephfs_{{ _ceph.cifmw_ceph_client_cluster }}
          share_driver = manila.share.drivers.cephfs.driver.CephFSDriver
          cephfs_conf_path = /etc/ceph/{{ _ceph.cifmw_ceph_client_cluster }}.conf
          cephfs_cluster_name = {{ _ceph.cifmw_ceph_client_cluster }}
          cephfs_auth_id=openstack
          cephfs_volume_mode = 0755
          cephfs_protocol_helper_type = CEPHFS
          backend_availability_zone = {{ _ceph.cifmw_ceph_client_cluster }}
        networkAttachments:
          - storage
{% endif %}
{% endfor %}
  neutron:
      template:
        customServiceConfig: |
{% if cifmw_ci_dcn_site_enable_network_az is true %}
          [DEFAULT]
          router_scheduler_driver = neutron.scheduler.l3_agent_scheduler.AZLeastRoutersScheduler
          network_scheduler_driver =  neutron.scheduler.dhcp_agent_scheduler.AZAwareWeightScheduler
          default_availability_zones = az0,az1,az2
{% endif %}
          [ml2_type_vlan]
          network_vlan_ranges = datacentre:1:1000,leaf1:1:1000,leaf2:1:1000
          [neutron]
          physnets = datacentre,leaf1,leaf2
  ovn:
    template:
      ovnController:
        external-ids:
          enable-chassis-as-gateway: true
          ovn-bridge: br-int
          ovn-encap-type: geneve
          system-id: random
{% if cifmw_ci_dcn_site_enable_network_az is true %}
          availability-zones:
            - az0
{% else %}
          availability-zones: []
{% endif %}
  nova:
      customServiceConfig: |
        [DEFAULT]
        default_schedule_zone=az0
        [cinder]
        cross_az_attach=False
      metadataServiceTemplate:
        enabled: false
      cellTemplates:
        cell0:
          cellDatabaseAccount: nova-cell0
          hasAPIAccess: true
{% for index in range(1, _all_azs | length + 1) %}
{% if "az" ~ (index - 1) != _az_to_scaledown %}
        cell{{ index }}:
          cellDatabaseInstance: openstack-cell{{ index }}
          cellDatabaseAccount: nova-cell{{ index }}
          cellMessageBusInstance: rabbitmq-cell{{ index }}
          conductorServiceTemplate:
            replicas: 1
          hasAPIAccess: true
          metadataServiceTemplate:
            enabled: true
            override:
              service:
                metadata:
                  annotations:
                    metallb.universe.tf/address-pool: internalapi
                    metallb.universe.tf/allow-shared-ip: internalapi
                    metallb.universe.tf/loadBalancerIPs: 172.17.0.8{{ index }}
                spec:
                  type: LoadBalancer
            replicas: 3
{% endif %}
{% endfor %}
  rabbitmq:
    templates:
      rabbitmq:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.85
            spec:
              type: LoadBalancer
        replicas: 3
{% for index in range(1, _all_azs | length + 1) %}
{% if "az" ~ (index - 1) != _az_to_scaledown %}
      rabbitmq-cell{{ index }}:
        override:
          service:
            metadata:
              annotations:
                metallb.universe.tf/address-pool: internalapi
                metallb.universe.tf/loadBalancerIPs: 172.17.0.8{{ 5 + index }}
            spec:
              type: LoadBalancer
        replicas: 3
{% endif %}
{% endfor %}
  extraMounts:
    - name: v1
      region: r1
      extraVol:
        - propagation:
            - az0
            - CinderBackup
          extraVolType: Ceph
          volumes:
            - name: ceph
              secret:
                secretName: ceph-conf-files
          mounts:
            - name: ceph
              mountPath: /etc/ceph
              readOnly: true
{% for _ceph in _ceph_vars_list %}
{% if _ceph.cifmw_ceph_client_cluster != 'az0' and _ceph.cifmw_ceph_client_cluster != _az_to_scaledown %}
        - propagation:
            - {{ _ceph.cifmw_ceph_client_cluster }}
          extraVolType: Ceph
          volumes:
            - name: ceph-{{ _ceph.cifmw_ceph_client_cluster }}
              secret:
                secretName: ceph-conf-files-{{ _ceph.cifmw_ceph_client_cluster }}
          mounts:
            - name: ceph-{{ _ceph.cifmw_ceph_client_cluster }}
              mountPath: /etc/ceph
              readOnly: true
{% endif %}
{% endfor %}
