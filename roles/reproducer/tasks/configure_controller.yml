---
- name: Set facts related to the reproducer
  ansible.builtin.set_fact:
    _ctl_reproducer_basedir: >-
      {{
        (
         '/home/zuul',
         'ci-framework-data',
         ) | path_join
      }}

# The dynamic inventory sets the ansible_ssh_user to zuul once we get the proper
# ssh configuration accesses set.
- name: Configure controller-0
  when:
    - (
        cifmw_libvirt_manager_configuration.vms.controller.target is defined and
        cifmw_libvirt_manager_configuration.vms.controller.target == inventory_hostname
      ) or
      cifmw_libvirt_manager_configuration.vms.controller.target is undefined
  delegate_to: controller-0
  delegate_facts: false
  block:
    - name: Ensure directories exist
      ansible.builtin.file:
        path: "{{ _ctl_reproducer_basedir }}/{{ item }}"
        state: directory
        mode: "0755"
      loop:
        - parameters
        - artifacts

    - name: Build job inventory for hook usage
      tags:
        - bootstrap
      ansible.builtin.shell:
        cmd: >-
          cat /home/zuul/reproducer-inventory/* >
          {{  _ctl_reproducer_basedir }}/artifacts/zuul_inventory.yml

    # You want to use the "name" parameter of the ansible.builtin.include_vars
    # call, such as:
    # - name: Load mac mapping
    #   ansible.builtin.include_vars:
    #     file: "{{ _ctl_reproducer_basedir }}/parameters/interfaces-info.yml"
    #     name: my_fancy_name
    # Then you'll be able to access the mapping content via `my_fancy_name`.
    - name: Push the MAC mapping data
      tags:
        - bootstrap
      ansible.builtin.copy:
        dest: "{{ _ctl_reproducer_basedir }}/parameters/interfaces-info.yml"
        content: "{{ cifmw_libvirt_manager_mac_map | to_nice_yaml }}"

    - name: Inject other Hypervisor SSH keys
      when:
        - hostvars[host]['priv_ssh_key'] is defined
      vars:
        _ssh_key: "{{ hostvars[host]['priv_ssh_key']['content'] | b64decode }}"
        _ssh_host: >-
          {{
            hostvars[host]['ansible_host'] |
            default(hostvars[host]['inventory_hostname'])
          }}
      ansible.builtin.copy:
        dest: "/home/zuul/.ssh/ssh_{{ _ssh_host }}"
        content: "{{ _ssh_key }}"
        mode: "0600"
      loop: "{{ hostvars.keys() }}"
      loop_control:
        label: "{{ host }}"
        loop_var: "host"

    # We need to configure SSH on controller-0 so that
    # it will consume the right ssh key and not obsess on
    # remote host key checking. We have to put this in the
    # local .ssh/config, because ProxyJump doesn't take the
    # command line options, meaning ssh won't consume the various
    # options set in the ansible inventory.
    - name: Inject remote hypervisor SSH configuration
      when:
        - hostvars[host]['priv_ssh_key'] is defined
      vars:
        _ssh_host: >-
          {{
            hostvars[host]['ansible_host'] |
            default(hostvars[host]['inventory_hostname'])
          }}
      ansible.builtin.blockinfile:
        create: true
        path: "/home/zuul/.ssh/config"
        marker: "## {mark} {{ _ssh_host }}"
        block: |-
          Host {{ _ssh_host }} {{ hostvars[host]['inventory_hostname'] }}
            IdentityFile ~/.ssh/ssh_{{ _ssh_host }}
            StrictHostKeyChecking no
            UserKnownHostsFile /dev/null
            User {{ hostvars[host]['ansible_user'] | default(ansible_user_id) }}
      loop: "{{ hostvars.keys() }}"
      loop_control:
        label: "{{ host }}"
        loop_var: "host"

    - name: Inject ProxyJump configuration for remote hypervisor VMs
      when:
        - hostvars[host]['ansible_host'] is defined
      vars:
        _vm_type: "{{ host | regex_replace('\\-[0-9]*', '') }}"
        _vm_data: "{{ cifmw_libvirt_manager_configuration['vms'][_vm_type] }}"
        _proxy_user: >-
          {{
            (hostvars[_vm_data.target]['ansible_user'] |
            default(ansible_user_id)) |
            default('')
          }}
        _proxy_host: "{{ _vm_data.target | default('') }}"
        _ssh_host: >-
          {{
            (hostvars[_proxy_host]['ansible_host'] |
            default(hostvars[_proxy_host]['inventory_hostname'])) |
            default('')
          }}
        _vm_ip: "{{ hostvars[host]['ansible_host'] | default('')}}"
      ansible.builtin.blockinfile:
        path: "/home/zuul/.ssh/config"
        marker: "## {mark} {{ host }}"
        block: |-
          Host {{ host }} {{ _vm_ip }}
            Hostname {{ _vm_ip }}
            User {{ _vm_data['admin_user'] | default('zuul') }}
            StrictHostKeyChecking no
            UserKnownHostsFile /dev/null
          {% if _vm_data.target is defined and
             _vm_data.target != _layout.vms.controller.target %}
            IdentityFile ~/.ssh/ssh_{{ _ssh_host }}
            ProxyJump {{ _proxy_user }}@{{ _proxy_host }}
          {% elif host is match('^ocp.*') %}
            IdentityFile ~/.ssh/devscripts_key
          {% else %}
            IdentityFile ~/.ssh/id_cifw
          {% endif %}
      loop: "{{ hostvars.keys() }}"
      loop_control:
        loop_var: host
        label: "{{ host }}"

    - name: Create kube directory
      ansible.builtin.file:
        path: "/home/zuul/.kube"
        state: directory
        owner: zuul
        group: zuul
        mode: "0755"
    - name: Inject kubeconfig content
      when:
        - _devscripts_kubeconfig is defined or _crc_kubeconfig is defined
      ansible.builtin.copy:
        dest: "/home/zuul/.kube/config"
        content: >-
          {{
            (cifmw_use_devscripts | default(false) | bool) |
            ternary(_devscripts_kubeconfig.content, _crc_kubeconfig.content) |
            b64decode
          }}
        owner: zuul
        group: zuul
        mode: "0644"

    - name: Inject kubeadmin-password if exists
      when:
        - _devscripts_kubeadm.content is defined
      ansible.builtin.copy:
        dest: "/home/zuul/.kube/kubeadmin-password"
        content: "{{ _devscripts_kubeadm.content | b64decode }}"
        owner: zuul
        group: zuul
        mode: "0600"

    - name: Inject devscripts private key if set
      when:
        - _devscript_privkey.content is defined
      ansible.builtin.copy:
        dest: "/home/zuul/.ssh/devscripts_key"
        content: "{{ _devscript_privkey.content | b64decode }}"
        owner: "zuul"
        group: "zuul"
        mode: "0400"

    - name: Ensure /etc/ci/env is created
      become: true
      ansible.builtin.file:
        path: /etc/ci/env
        state: directory
        mode: "0755"

    - name: Tweak dnf configuration
      become: true
      community.general.ini_file:
        no_extra_spaces: true
        option: "{{ config.option }}"
        path: "/etc/dnf/dnf.conf"
        section: "{{ config.section | default('main') }}"
        state: "{{ config.state | default(omit) }}"
        value: "{{ config.value | default(omit) }}"
      loop: "{{ cifmw_reproducer_dnf_tweaks }}"
      loop_control:
        label: "{{ config.option }}"
        loop_var: 'config'

    - name: Install custom CA if needed
      ansible.builtin.import_role:
        name: install_ca

    - name: RHEL repository setup for ansible-controller
      become: true
      when:
        - cifmw_repo_setup_rhos_release_rpm is defined
      block:
        - name: Get rhos-release and CA
          ansible.builtin.import_tasks: rhos_release.yml

        - name: Create bundle for CRC
          ansible.builtin.shell:
            cmd: >-
              set -o pipefail;
              cat /etc/pki/ca-trust/source/anchors/* >
              /etc/pki/ca-trust/source/anchors/rh.crt
            creates: "/etc/pki/ca-trust/source/anchors/rh.crt"

    - name: Install some tools
      become: true
      ansible.builtin.package:
        name:
          - bash-completion
          - git-core
          - make
          - podman
          - python3-jmespath
          - python3-netaddr
          - python3-pip
          - tmux
          - vim
          - wget
          - jq

    - name: Install ansible dependencies
      ansible.builtin.pip:
        requirements: https://raw.githubusercontent.com/openstack-k8s-operators/ci-framework/main/common-requirements.txt

    - name: Inject most of the cifmw_ parameters passed to the reproducer run
      vars:
        _filtered_vars: >-
          {{
            hostvars[inventory_hostname] | default({}) |
            dict2items |
            selectattr('key', 'match',
                       '^cifmw_(?!install_yamls|openshift|devscripts).*') |
            rejectattr('key', 'equalto', 'cifmw_target_host') |
            rejectattr('key', 'equalto', 'cifmw_basedir') |
            rejectattr('key', 'equalto', 'cifmw_path') |
            rejectattr('key', 'equalto', 'cifmw_extras') |
            rejectattr('key', 'match', '^cifmw_use.*') |
            rejectattr('key', 'match', '^cifmw_reproducer.*') |
            rejectattr('key', 'match', '^cifmw_rhol.*') |
            rejectattr('key', 'match', '^cifmw_discover.*') |
            rejectattr('key', 'match', '^cifmw_libvirt_manager.*') |
            items2dict
          }}
      ansible.builtin.copy:
        dest: "/home/zuul/reproducer-variables.yml"
        content: "{{ _filtered_vars | to_nice_yaml }}"

    - name: Get interfaces-info content
      register: _nic_info
      ansible.builtin.slurp:
        src: "{{ _ctl_reproducer_basedir }}/parameters/interfaces-info.yml"

    # We detected OCP cluster may have some downtime even after it's supposed
    # to be started.
    # It means everything was right from the libvirt_manager point of view, it
    # could get access, configure the needed bits and move on.
    # But when we hit this step, cluster may be unresponsive - suspecting
    # some NetworkManager or related tool doing some "late" tasks.
    # This task here is to ensure all of the OCPs nodes are ready to be
    # consumed by the networking_mapper - it will connect to all of the nodes
    # in order to gather some facts of interest.
    - name: Wait for OCP nodes to be ready
      delegate_to: "{{ item }}"
      ansible.builtin.wait_for_connection:
        sleep: 2
        timeout: 300
      loop: "{{ groups.ocps }}"

    - name: Generate networking definition
      vars:
        cifmw_networking_mapper_ifaces_info: >-
          {{ _nic_info.content | b64decode | from_yaml }}
        cifmw_networking_mapper_network_name: >-
          {{ _layout.vms.controller.nets.1 }}
      ansible.builtin.import_role:
        name: networking_mapper

    - name: Configure ctlplane interface on controller-0
      become: true
      vars:
        _ctlplane_net: "{{ cifmw_networking_env_definition.networks.ctlplane }}"
        _controller_data: "{{ cifmw_networking_env_definition.instances['controller-0'].networks.ctlplane}}"
        _prefix: "{{ _ctlplane_net.network_v4 | ansible.utils.ipaddr('prefix') }}"
      community.general.nmcli:
        autoconnect: true
        conn_name: ctlplane
        dns4: "{{ _ctlplane_net.dns_v4.0 }}"
        ifname: "{{ _controller_data.interface_name }}"
        type: ethernet
        ip4: "{{ _controller_data.ip_v4 }}/{{ _prefix }}"
        never_default4: true
        state: present

    - name: Inject CRC related content if needed
      when:
        - _layout.vms.crc is defined
      block:
        - name: Inject CRC ssh key
          ansible.builtin.copy:
            dest: "/home/zuul/.ssh/crc_key"
            content: "{{ crc_priv_key['content'] | b64decode }}"
            mode: "0400"
            owner: zuul
            group: zuul

        - name: Inject crc related host entry
          become: true
          vars:
            _crc_data: "{{ cifmw_networking_env_definition.instances['crc-0'].networks.ctlplane}}"
          ansible.builtin.lineinfile:
            path: /etc/hosts
            line: >-
              {{ _crc_data.ip_v4 }} api.crc.testing
              canary-openshift-ingress-canary.apps-crc.testing
              console-openshift-console.apps-crc.testing
              default-route-openshift-image-registry.apps-crc.testing
              downloads-openshift-console.apps-crc.testing
              oauth-openshift.apps-crc.testing
