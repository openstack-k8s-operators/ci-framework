---
# This is local to your desktop/laptop.
# We can't use ansible_user_dir here, unless you have the same user on the
# hypervisor and locally.
cifmw_install_yamls_repo: "{{ ansible_user_dir }}/src/github.com/openstack-k8s-operators/install_yamls"
# This will be created on the hypervisor.
cifmw_basedir: "{{ ansible_user_dir }}/ci-framework-data"
cifmw_path: "{{ ansible_user_dir }}/bin:{{ ansible_env.PATH }}"

cifmw_reproducer_epel_pkgs:
  - python3-bcrypt
  - python3-passlib

cifmw_libvirt_manager_pub_net: ocpbm

# Automation section. Most of those parameters will be passed to the
# controller-0 as-is and be consumed by the `deploy-va.sh` script.
# Please note, all paths are on the controller-0, meaning managed by the
# Framework. Please do not edit them!
_arch_repo: "/home/zuul/src/github.com/openstack-k8s-operators/architecture"
cifmw_architecture_scenario: hci
cifmw_ceph_client_vars: /tmp/ceph_client.yml
cifmw_ceph_client_values_post_ceph_path_src: >-
  {{ _arch_repo }}/examples/va/hci/values.yaml
cifmw_ceph_client_values_post_ceph_path_dst: >-
  {{ cifmw_ceph_client_values_post_ceph_path_src }}
cifmw_ceph_client_service_values_post_ceph_path_src: >-
  {{ _arch_repo }}/examples/va/hci/service-values.yaml
cifmw_ceph_client_service_values_post_ceph_path_dst: >-
  {{ cifmw_ceph_client_service_values_post_ceph_path_src }}

# HERE if you want to overload kustomization, you can uncomment this parameter
# and push the data structure you want to apply.
# cifmw_architecture_user_kustomize:
#   stage_0:
#     'network-values':
#       data:
#         starwars: Obiwan

# HERE, if you want to stop the deployment loop at any stage, you can uncomment
# the following parameter and update the value to match the stage you want to
# reach. Known stages are:
#   pre_kustomize_stage_INDEX
#   pre_apply_stage_INDEX
#   post_apply_stage_INDEX
#
# cifmw_deploy_architecture_stopper:

# What about some tempest?
cifmw_run_tests: true
cifmw_run_tempest: true
cifmw_run_test_role: test_operator
cifmw_test_operator_timeout: 7200
cifmw_test_operator_tempest_include_list: |
  tempest.scenario.test_network_basic_ops.TestNetworkBasicOps

# This will instruct libvirt_manager to create 3 compute and
# the ansible-controller, consuming the networks created and
# managed by devscripts.
# Note that the "osp_trunk" network is the equivalent of the
# "private_net" in the CI layout, and will hold the VLAN for
# network isolation.
# The "ocpbm" network is managed by devscripts as well, and
# provides access to Internet. This will be the equivalent of the
# "public network" as seen in CI.
cifmw_use_libvirt: true
cifmw_virtualbmc_daemon_port: 50881
cifmw_use_uefi: >-
  {{ (cifmw_repo_setup_os_release is defined
      and cifmw_repo_setup_os_release == 'rhel') | bool }}
cifmw_root_partition_id: >-
  {{
    (cifmw_repo_setup_os_release is defined and cifmw_repo_setup_os_release == 'rhel') |
    ternary(4, 1)
  }}
cifmw_libvirt_manager_compute_amount: 3
cifmw_libvirt_manager_configuration:
  networks:
    osp_trunk: |
      <network>
        <name>osp_trunk</name>
        <forward mode='nat'/>
        <bridge name='osp_trunk' stp='on' delay='0'/>
        <ip family='ipv4'
        address='{{ cifmw_networking_definition.networks.ctlplane.network |
                    ansible.utils.nthhost(1) }}'
        prefix='{{ cifmw_networking_definition.networks.ctlplane.network |
                   ansible.utils.ipaddr('prefix') }}'>
        </ip>
      </network>
  vms:
    compute:
      uefi: "{{ cifmw_use_uefi }}"
      root_part_id: "{{ cifmw_root_partition_id }}"
      amount: "{{ [cifmw_libvirt_manager_compute_amount|int, 3] |Â max }}"
      image_url: "{{ cifmw_discovered_image_url }}"
      sha256_image_name: "{{ cifmw_discovered_hash }}"
      image_local_dir: "{{ cifmw_basedir }}/images/"
      disk_file_name: "base-os.qcow2"
      disksize: "{{ [cifmw_libvirt_manager_compute_disksize|int, 50] | max }}"
      memory: "{{ [cifmw_libvirt_manager_compute_memory|int, 8] | max }}"
      cpus: "{{ [cifmw_libvirt_manager_compute_cpus|int, 4] | max }}"
      nets:
        - ocpbm
        - osp_trunk
    controller:
      uefi: "{{ cifmw_use_uefi }}"
      root_part_id: "{{ cifmw_root_partition_id }}"
      image_url: "{{ cifmw_discovered_image_url }}"
      sha256_image_name: "{{ cifmw_discovered_hash }}"
      image_local_dir: "{{ cifmw_basedir }}/images/"
      disk_file_name: "base-os.qcow2"
      disksize: 50
      memory: 8
      cpus: 4
      nets:
        - ocpbm
        - osp_trunk
    ocp:
      amount: 3
      admin_user: core
      image_local_dir: "/home/dev-scripts/pool"
      disk_file_name: "ocp_master"
      disksize: "105"
      xml_paths:
        - /home/dev-scripts/ocp_master_0.xml
        - /home/dev-scripts/ocp_master_1.xml
        - /home/dev-scripts/ocp_master_2.xml
      nets:
        - osp_trunk


## devscript support for OCP deploy
cifmw_use_devscripts: true
cifmw_devscripts_config_overrides:
  worker_memory: 16384
  worker_disk: 100
  worker_vcpu: 10
  num_extra_workers: 0
# Required for egress traffic from pods to the osp_trunk network
cifmw_devscripts_enable_ocp_nodes_host_routing: true

# Note: with that extra_network_names "osp_trunk", we instruct
# devscripts role to create a new network, and associate it to
# the OCP nodes. This one is a "private network", and will hold
# the VLANs used for network isolation.

# Please create a custom env file to provide:
# cifmw_devscripts_ci_token:
# cifmw_devscripts_pull_secret:

# type and size of ssh keys injected into the OCP workers and compute nodes
cifmw_ssh_keytype: ecdsa
cifmw_ssh_keysize: 521

# Test Ceph file and object storage (block is enabled by default)
cifmw_ceph_daemons_layout:
  rgw_enabled: true
  dashboard_enabled: false
  cephfs_enabled: true
