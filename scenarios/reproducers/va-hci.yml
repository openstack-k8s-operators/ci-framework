---
cifmw_parent_scenario: "scenarios/reproducers/va-hci-base.yml"

# HERE if you want to override kustomization, you can uncomment this parameter
# and push the data structure you want to apply.
# cifmw_architecture_user_kustomize:
#   stage_0:
#     'network-values':
#       data:
#         starwars: Obiwan

# HERE, if you want to stop the deployment loop at any stage, you can uncomment
# the following parameter and update the value to match the stage you want to
# reach. Known stages are:
#   pre_kustomize_stage_INDEX
#   pre_apply_stage_INDEX
#   post_apply_stage_INDEX
#
# cifmw_deploy_architecture_stopper:

cifmw_libvirt_manager_configuration:
  networks:
    osp_trunk: |
      <network>
        <name>osp_trunk</name>
        <forward mode='nat'/>
        <bridge name='osp_trunk' stp='on' delay='0'/>
        <dns enable="no"/>
        <ip family='ipv4'
        address='{{ cifmw_networking_definition.networks.ctlplane.network |
                    ansible.utils.nthhost(1) }}'
        prefix='{{ cifmw_networking_definition.networks.ctlplane.network |
                   ansible.utils.ipaddr('prefix') }}'>
        </ip>
      </network>
    ocpbm: |
      <network>
        <name>ocpbm</name>
        <forward mode='nat'/>
        <bridge name='ocpbm' stp='on' delay='0'/>
        <dns enable="no"/>
        <ip family='ipv4' address='192.168.111.1' prefix='24'>
        </ip>
      </network>
    ocppr: |
      <network>
        <name>ocppr</name>
        <forward mode='bridge'/>
        <bridge name='ocppr'/>
      </network>
  vms:
    ocp:
      amount: 3
      admin_user: core
      image_local_dir: "{{ cifmw_basedir }}/images/"
      disk_file_name: "ocp_master"
      disksize: "100"
      extra_disks_num: 3
      extra_disks_size: "50G"
      cpus: 16
      memory: 32
      root_part_id: 4
      uefi: true
      nets:
        - ocppr
        - ocpbm
        - osp_trunk
    compute:
      uefi: "{{ cifmw_use_uefi }}"
      root_part_id: "{{ cifmw_root_partition_id }}"
      amount: "{{ [cifmw_libvirt_manager_compute_amount|int, 3] |Â max }}"
      image_url: "{{ cifmw_discovered_image_url }}"
      sha256_image_name: "{{ cifmw_discovered_hash }}"
      image_local_dir: "{{ cifmw_basedir }}/images/"
      disk_file_name: "base-os.qcow2"
      disksize: "{{ [cifmw_libvirt_manager_compute_disksize|int, 50] | max }}"
      memory: "{{ [cifmw_libvirt_manager_compute_memory|int, 8] | max }}"
      cpus: "{{ [cifmw_libvirt_manager_compute_cpus|int, 4] | max }}"
      extra_disks_num: 3
      extra_disks_size: 30G
      nets:
        - ocpbm
        - osp_trunk
    controller:
      uefi: "{{ cifmw_use_uefi }}"
      root_part_id: "{{ cifmw_root_partition_id }}"
      image_url: "{{ cifmw_discovered_image_url }}"
      sha256_image_name: "{{ cifmw_discovered_hash }}"
      image_local_dir: "{{ cifmw_basedir }}/images/"
      disk_file_name: "base-os.qcow2"
      disksize: 50
      memory: 8
      cpus: 4
      nets:
        - ocpbm
        - osp_trunk

## devscript support for OCP deploy
cifmw_devscripts_config_overrides:
  fips_mode: "{{ cifmw_fips_enabled | default(false) | bool }}"

# Note: with that extra_network_names "osp_trunk", we instruct
# devscripts role to create a new network, and associate it to
# the OCP nodes. This one is a "private network", and will hold
# the VLANs used for network isolation.

# Please create a custom env file to provide:
# cifmw_devscripts_ci_token:
# cifmw_devscripts_pull_secret:

# Test Ceph file and object storage (block is enabled by default)
cifmw_ceph_daemons_layout:
  rgw_enabled: true
  dashboard_enabled: false
  cephfs_enabled: true
  ceph_nfs_enabled: false
