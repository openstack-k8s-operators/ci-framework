---
cifmw_architecture_scenario: osasinfra
cifmw_arch_automation_file: osasinfra.yaml

# Automation section. Most of those parameters will be passed to the
# controller-0 as-is and be consumed by the `deploy-va.sh` script.
# Please note, all paths are on the controller-0, meaning managed by the
# Framework. Please do not edit them!
_arch_repo: "/home/zuul/src/github.com/openstack-k8s-operators/architecture"
cifmw_ceph_client_vars: /tmp/ceph_client.yml
cifmw_ceph_client_values_post_ceph_path_src: >-
  {{ _arch_repo }}/examples/dt/osasinfra/values.yaml
cifmw_ceph_client_values_post_ceph_path_dst: >-
  {{ cifmw_ceph_client_values_post_ceph_path_src }}
cifmw_ceph_client_service_values_post_ceph_path_src: >-
  {{ _arch_repo }}/examples/dt/osasinfra/service-values.yaml
cifmw_ceph_client_service_values_post_ceph_path_dst: >-
  {{ cifmw_ceph_client_service_values_post_ceph_path_src }}


# workaround https://issues.redhat.com/browse/OSPRH-6675
cifmw_ceph_spec_public_network: "{{ cifmw_networking_definition.networks.ctlplane.network }}"

# HERE if you want to overload kustomization, you can uncomment this parameter
# and push the data structure you want to apply.
# cifmw_architecture_user_kustomize:
#   stage_0:
#     'network-values':
#       data:
#         starwars: Obiwan

# HERE, if you want to stop the deployment loop at any stage, you can uncomment
# the following parameter and update the value to match the stage you want to
# reach. Known stages are:
#   pre_kustomize_stage_INDEX
#   pre_apply_stage_INDEX
#   post_apply_stage_INDEX
#
# cifmw_deploy_architecture_stopper:

cifmw_allow_vms_to_reach_osp_api: true
cifmw_networking_mapper_definition_patches_01:
  networks:
    tenant:
      mtu: 1500

# HCI requires bigger size to hold OCP on OSP disks
cifmw_block_device_size: 100G
cifmw_libvirt_manager_compute_disksize: 200
cifmw_libvirt_manager_compute_memory: 50
cifmw_libvirt_manager_compute_cpus: 8

cifmw_libvirt_manager_configuration:
  networks:
    osp_trunk: |
      <network>
        <name>osp_trunk</name>
        <forward mode='nat'/>
        <bridge name='osp_trunk' stp='on' delay='0'/>
        <ip family='ipv4'
        address='{{ cifmw_networking_definition.networks.ctlplane.network |
                    ansible.utils.nthhost(1) }}'
        prefix='{{ cifmw_networking_definition.networks.ctlplane.network |
                   ansible.utils.ipaddr('prefix') }}'>
        </ip>
      </network>
  vms:
    ocp:
      amount: 3
      admin_user: core
      image_local_dir: "/home/dev-scripts/pool"
      disk_file_name: "ocp_master"
      disksize: "105"
      xml_paths:
        - /home/dev-scripts/ocp_master_0.xml
        - /home/dev-scripts/ocp_master_1.xml
        - /home/dev-scripts/ocp_master_2.xml
      nets:
        - osp_trunk
    compute:
      uefi: "{{ cifmw_use_uefi }}"
      root_part_id: "{{ cifmw_root_partition_id }}"
      amount: "{{ [cifmw_libvirt_manager_compute_amount|int, 3] |Â max }}"
      image_url: "{{ cifmw_discovered_image_url }}"
      sha256_image_name: "{{ cifmw_discovered_hash }}"
      image_local_dir: "{{ cifmw_basedir }}/images/"
      disk_file_name: "base-os.qcow2"
      disksize: "{{ [cifmw_libvirt_manager_compute_disksize|int, 50] | max }}"
      memory: "{{ [cifmw_libvirt_manager_compute_memory|int, 8] | max }}"
      cpus: "{{ [cifmw_libvirt_manager_compute_cpus|int, 4] | max }}"
      nets:
        - ocpbm
        - osp_trunk
    controller:
      uefi: "{{ cifmw_use_uefi }}"
      root_part_id: "{{ cifmw_root_partition_id }}"
      image_url: "{{ cifmw_discovered_image_url }}"
      sha256_image_name: "{{ cifmw_discovered_hash }}"
      image_local_dir: "{{ cifmw_basedir }}/images/"
      disk_file_name: "base-os.qcow2"
      disksize: 50
      memory: 8
      cpus: 4
      nets:
        - ocpbm
        - osp_trunk

## devscript support for OCP deploy
cifmw_devscripts_config_overrides:
  worker_memory: 16384
  worker_disk: 100
  worker_vcpu: 10
  num_extra_workers: 0
  fips_mode: "{{ cifmw_fips_enabled | default(false) | bool }}"

# Note: with that extra_network_names "osp_trunk", we instruct
# devscripts role to create a new network, and associate it to
# the OCP nodes. This one is a "private network", and will hold
# the VLANs used for network isolation.

# Please create a custom env file to provide:
# cifmw_devscripts_ci_token:
# cifmw_devscripts_pull_secret:

# Test Ceph file and object storage (block is enabled by default)
cifmw_ceph_daemons_layout:
  rgw_enabled: true
  dashboard_enabled: false
  cephfs_enabled: true
  ceph_nfs_enabled: true
